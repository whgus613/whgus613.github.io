---
title: "[ML] epoch, batch size, iteration"
layout: post
---

### epoch, batch size, iteration

머신 러닝에서 최적화를 할 때는 여러 번의 학습을 하는 것이 일반적이다. 한 번의 학습 과정 역시 데이터를 나누는 방식에 따라 세분화된다. 이때 사용되는 개념이 epoch, batch size, iteration이다. 이번 포스팅에서는 각 개념에 대해 알아볼 것이다.


#### epoch

epoch는 훈련 데이터셋에 포함된 모든 데이터들이 모델을 통과한 횟수를 말한다. epoch가 1이라면 전체 학습 데이터셋이 한 신경망에 적용돼 신경망을 한 번 통과했다는 의미가 된다. epoch가 높아질수록 다양한 무작위 가중치로 학습을 할 수 있어 적합한 파라미터를 찾을 확률이 올라간다. 그러나 epoch를 지나치게 높게 설정한다면 학습 데이터셋에 과적합돼 다른 데이터에 대해선 제대로 된 예측을 하지 못할 수 있다.

#### batch size

batch size는 전체 학습 데이터셋을 여러 그룹으로 나누었을 때 하나의 소그룹에 속하는 데이터 수를 의미한다. 학습 데이터를 통째로 신경망에 넣으면 비효율적인 자원 사용으로 학습 시간이 지연될 수 있어 전체 학습 데이터를 작게 나눠야 한다. batch size가 너무 크면 한 번에 처리해야 할 데이터의 양이 많아지므로 학습 속도가 느려지고 메모리 부족 문제가 발생할 수 있다. 그러나 batch size가 너무 작으면 적은 데이터를 대상으로 가중치를 업데이트하고, 이 업데이트가 자주 발생하기 때문에 훈련이 불안정해진다.

#### iteration

iteration은 1-epoch를 마치는 데 필요한 batch의 수를 말한다. 1-epoch를 마치는 데 필요한 파라미터 업데이트 횟수라고 이해하면 된다. 각 batch마다 파라미터 업데이트가 한 번씩 이뤄지므로, iteration은 파라미터 업데이트 횟수이자 batch size의 개수이다.
